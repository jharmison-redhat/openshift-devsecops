---
# This tells the playbooks to not try to include default-named vars files
common_included: yes


################################################################################
# The following variables are especially important to pay attention to if you're
# using an RHPDS-provisioned cluster and some of it needs to be updated every
# time you provision a cluster to match information from the email you receive.
#
# NOTE: IF A VARIABLE IS COMMENTED, IT LIKELY DEFAULTS TO THE OPPOSITE
################################################################################

# Uncomment the following for RHPDS-provisioned clusters because right now we
#   require AWS creds, though this may change in the future. RHPDS provides an
#   option to do letsencrypt on the api and default router certs, but doesn't
#   give us the ability to do it for follow-on tasks that won't use the router
#   cert.
# letsencrypt: no     # Creates a LetsEncrypt wildcard and api certificate and makes it the default

# The name of the cluster.
# IF PROVISIONING:
#   This value will be in your DNS entries and should conform to valid DNS characters.
# IF RHPDS:
#   This should be the first part of your cluster's base address
#   e.g. for cluster-nisky-c592.nisky-c592.example.opentlc.com, this would be
#         cluster-nisky-c592
cluster_name: openshift

# The base subdomain to use for your cluster.
# IF PROVISIONING:
#   Example: If you set this to `example.com`, a DNS entry for `<cluster_name>.example.com` will be created
# IF RHPDS:
#   This should be the rest of your cluster's base address
openshift_base_domain: example.com

################################################################################
# DO NOT CHANGE OR COMMENT THESE FOR ANY REASON - they're used heavily
#   throughout all the various roles
_tmp_parent: '{{ "/".join([ playbook_dir, "../tmp" ])|realpath }}'
full_cluster_name: '{{ ".".join([ cluster_name, openshift_base_domain ]) }}'
tmp_dir: '{{ "/".join([ _tmp_parent, full_cluster_name ]) }}'
################################################################################

# The path to the kubeconfig file for the cluster you're using for the workshop.
#   The default value is what's configured if you're provisioning the workshop
#   in AWS, RHPDS clusters should use your user's kubeconfig after caching login
#   credentials with oc.
# NOTE: Do not use ~ or other shell-expandable variables
# kubeconfig: '{{ ansible_env["HOME"] }}/.kube/config'
kubeconfig: '{{ tmp_dir }}/auth/kubeconfig'

# The path to your 'oc' client - the provisioner puts it in {{ tmp_dir }},
#   for RHPDS clusters you should specify the absolute path of your oc client.
# oc_cli: '/usr/local/bin/oc'
oc_cli: '{{ tmp_dir }}/oc'


################################################################################
# The remaining variables are to customize your deployment configuration beyond
#   what is required for the workshop to function. You should read through and
#   understand them, but they aren't necessary to minimally deploy.
################################################################################

# A list of manually created users:
manual_users:
  - username: openshift-admin
    password: RedHatAdmin1
    admin: yes

# A specially designated user for administering workshop content
workshop_admin:
  username: workshop-admin
  password: RedHatAdmin1
  admin: yes

# The number of users created, as a string
number_of_users: "30"

# Generate a sequence of users
sequence_users: |
  {%- for username in lookup("sequence", "1-" + number_of_users + ":user%0i", wantlist=True) %}
    - username: {{ username }}
      password: openshift
  {% endfor -%}

# The users to create in OpenShift
openshift_users: '{{ manual_users + [workshop_admin] + sequence_users|from_yaml }}'
# The users to create workshop projects/integrations for
workshop_users: '{{ [workshop_admin] + sequence_users|from_yaml }}'

# The type of worker to provision (including MachineSet adjustments)
cluster_worker_machine_type: m4.2xlarge
# The MachineSet replicas to enable on the MachineAutoscaler
machineset_min_replicas: 0
machineset_max_replicas: 5
# The number of nodes to limit the overall cluster to across all MachineSets
cluster_max_nodes_total: 15
# Uncomment the following to adjust the overall minimum/maximum resources for the entire cluster
# cluster_min_cores: 8
# cluster_max_cores: 100
# cluster_min_ram_gb: 32
# cluster_max_ram_gb: 256
